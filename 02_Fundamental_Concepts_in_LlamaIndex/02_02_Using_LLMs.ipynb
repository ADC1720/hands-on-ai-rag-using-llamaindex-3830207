{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ADC1720/hands-on-ai-rag-using-llamaindex-3830207/blob/main/02_Fundamental_Concepts_in_LlamaIndex/02_02_Using_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AbxxUMQZ6-o-"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install llama-index --upgrade\n",
        "!pip install llama-index-llms-cohere --upgrade\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "h8c-SZWL6-o_",
        "outputId": "c4f5f109-a8aa-4ed2-a808-15be5edc3dbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "from getpass import getpass\n",
        "import nest_asyncio\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tLeQehL96-o_"
      },
      "outputs": [],
      "source": [
        "CO_API_KEY = os.environ.get('JWE2OJ3FGW5b0RH3Y8qTURJWa5IvC7HL0tUMontO')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvXlFuHm6-o_"
      },
      "source": [
        "When building an LLM-based application, one of the first decisions you make is which LLM(s) to use (of course, you can use more than one if you wish).\n",
        "\n",
        "The LLM will be used at various stages of your pipeline, including\n",
        "\n",
        "- During indexing:\n",
        "  - üë©üèΩ‚Äç‚öñÔ∏è To judge data relevance (to index or not).\n",
        "  - üìñ Summarize data & index those summaries.\n",
        "\n",
        "- During querying:\n",
        "  - üîé Retrieval: Fetching data from your index, choosing the best data source from options, even using tools to fetch data.\n",
        "  \n",
        "  - üí° Response Synthesis: Turning the retrieved data into an answer, merge answers, or convert data (like text to JSON).\n",
        "\n",
        "LlamaIndex gives you a single interface to various LLMs. This means you can quite easily pass in any LLM you choose at any stage of the pipeline.\n",
        "\n",
        "In this course we'll primiarly use OpenAI. You can see a full list of LLM integrations [here](https://docs.llamaindex.ai/en/stable/module_guides/models/llms/modules.html) and use your LLM provider of choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ehs79HWx6-pA"
      },
      "source": [
        "# Basic Usage\n",
        "\n",
        "You can call `complete` with a prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VEgO3yuA6-pA",
        "outputId": "afebf849-1a24-40e3-b7df-877f0db273c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assistant: Alexander the Great, also known as Alexander III of Macedon, was a renowned ancient king and military commander. He is considered one of history's most successful military commanders, having never lost a battle. Born in 356 BCE in Pella, Macedonia (in present-day Greece), Alexander became king at the age of 20 after his father, Philip II, was assassinated.\n",
            "\n",
            "Alexander's most notable achievements include:\n",
            "\n",
            "1. **Conquests**: He expanded the Macedonian Empire across three continents, conquering the Persian Empire, Egypt, and parts of India. His empire stretched from Greece in the west to northwestern India in the east.\n",
            "\n",
            "2. **Military Genius**: Known for his tactical brilliance, Alexander led his armies to victories in numerous battles, including the famous battles of **Gaugamela** against the Persian king Darius III and **Hydaspes** against King Porus of India.\n",
            "\n",
            "3. **Cultural Impact**: Alexander's conquests facilitated the spread of Greek culture (Hellenism) across his vast empire, blending Greek, Persian, Egyptian, and Indian influences. This cultural exchange had a lasting impact on the regions he conquered.\n",
            "\n",
            "4. **Founding Cities**: He founded over 20 cities, the most famous being **Alexandria in Egypt**, which became a major center of learning and culture in the ancient world.\n",
            "\n",
            "5. **Legacy**: Despite his early death at the age of 32 in 323 BCE, Alexander's legacy endured. His empire fragmented after his death, but his influence on history, military strategy, and culture remains significant.\n",
            "\n",
            "Alexander the Great is often remembered as a symbol of leadership, ambition, and the fusion of diverse cultures.\n"
          ]
        }
      ],
      "source": [
        "# from llama_index.llms.cohere import Cohere\n",
        "\n",
        "# llm = Cohere(model=\"command-r-plus\", api_key = 'JWE2OJ3FGW5b0RH3Y8qTURJWa5IvC7HL0tUMontO', temperature=0.2)\n",
        "\n",
        "# response = llm.complete(\"Alexander the Great was a\")\n",
        "\n",
        "# print(response)\n",
        "\n",
        "\n",
        "from llama_index.llms.cohere import Cohere\n",
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "llm = Cohere(model=\"command-a-03-2025\", api_key='JWE2OJ3FGW5b0RH3Y8qTURJWa5IvC7HL0tUMontO', temperature=0.2)\n",
        "\n",
        "messages = [\n",
        "    ChatMessage(role=\"user\", content=\"Alexander the Great was a\")\n",
        "]\n",
        "\n",
        "response = llm.chat(messages)\n",
        "print(response)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAhmCTlV6-pA"
      },
      "source": [
        "# Prompt templates\n",
        "\n",
        "- ‚úçÔ∏è A prompt template is a fundamental input that gives LLMs their expressive power in the LlamaIndex framework.\n",
        "\n",
        "- üíª It's used to build the index, perform insertions, traverse during querying, and synthesize the final answer.\n",
        "\n",
        "- ü¶ô LlamaIndex has several built-in prompt templates.\n",
        "\n",
        "- üõ†Ô∏è Below is how you can create one from scratch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "rCZ0sCXj6-pA",
        "outputId": "8b3275fc-de93-43de-d537-de8a73db1e26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assistant: **\"Broken Xylophone Blues\"**  \n",
            "*(Parody Rap in the Style of Eminem‚Äôs \"Lose Yourself\")*  \n",
            "\n",
            "*[Beat drops, xylophone clangs awkwardly in the background]*  \n",
            "\n",
            "**Verse 1:**  \n",
            "Yo, it‚Äôs the story of a xylophone, once the life of the party,  \n",
            "Now it‚Äôs sittin‚Äô in the corner, soundin‚Äô like a fartin‚Äô Harley.  \n",
            "Bar 1‚Äôs cracked, bar 3‚Äôs gone, bar 5‚Äôs just a stub,  \n",
            "Tryna play a melody, but it‚Äôs soundin‚Äô like a dub.  \n",
            "Used to be the star of the orchestra, now it‚Äôs just a joke,  \n",
            "Every time I hit a note, it‚Äôs like, ‚ÄúYo, did you hear that stroke?‚Äù  \n",
            "Teacher‚Äôs like, ‚ÄúMaybe stick to drums, you‚Äôre killin‚Äô the vibe,‚Äù  \n",
            "But I‚Äôm like, ‚ÄúNah, I‚Äôm bringin‚Äô this xylophone back to life!‚Äù  \n",
            "\n",
            "**Chorus:**  \n",
            "It‚Äôs the broken xylophone blues, man, it‚Äôs a tragedy,  \n",
            "Every note‚Äôs a gamble, like a musical lottery.  \n",
            "I‚Äôm tryna fix it up, but the glue‚Äôs not holdin‚Äô tight,  \n",
            "Broken xylophone, but I‚Äôm still tryna make it right.  \n",
            "Broken xylophone, but I‚Äôm still tryna make it right.  \n",
            "\n",
            "**Verse 2:**  \n",
            "Went to the music store, dude‚Äôs like, ‚ÄúThat‚Äôll be a grand,‚Äù  \n",
            "I‚Äôm like, ‚ÄúFor a xylophone? You tryna rob this man?‚Äù  \n",
            "Tried to DIY, watched a YouTube tutorial,  \n",
            "Now there‚Äôs wood glue everywhere, and it‚Äôs still subpar, y‚Äôall.  \n",
            "Bar 7‚Äôs stuck, bar 9‚Äôs warped, bar 11‚Äôs just a mess,  \n",
            "Tryna play ‚ÄúTwinkle Twinkle,‚Äù but it sounds like distress.  \n",
            "Mom‚Äôs like, ‚ÄúMaybe it‚Äôs time to let it go, it‚Äôs had its run,‚Äù  \n",
            "But I‚Äôm like, ‚ÄúNah, this xylophone‚Äôs my second son!‚Äù  \n",
            "\n",
            "**Chorus:**  \n",
            "It‚Äôs the broken xylophone blues, man, it‚Äôs a tragedy,  \n",
            "Every note‚Äôs a gamble, like a musical lottery.  \n",
            "I‚Äôm tryna fix it up, but the glue‚Äôs not holdin‚Äô tight,  \n",
            "Broken xylophone, but I‚Äôm still tryna make it right.  \n",
            "Broken xylophone, but I‚Äôm still tryna make it right.  \n",
            "\n",
            "**Bridge:**  \n",
            "*(Spoken word, over a somber xylophone clang)*  \n",
            "You know, they say every instrument has a soul,  \n",
            "But this one‚Äôs soul is held together with tape and a prayer, I suppose.  \n",
            "Still, I‚Äôll keep bangin‚Äô on it, ‚Äòcause it‚Äôs all I got,  \n",
            "Broken xylophone, but it‚Äôs my imperfect shot.  \n",
            "\n",
            "**Verse 3:**  \n",
            "Performed at the school recital, thought I‚Äôd steal the show,  \n",
            "Hit the first note, and the whole crowd was like, ‚ÄúOh no.‚Äù  \n",
            "Bar 2 fell off, bar 4‚Äôs hangin‚Äô by a thread,  \n",
            "But I kept goin‚Äô, ‚Äòcause this xylophone‚Äôs my bread and butter, instead.  \n",
            "Ended with a crash, but you know what? I don‚Äôt care,  \n",
            "‚ÄòCause this broken xylophone‚Äôs still got flair.  \n",
            "So if you see me on the street with my duct-taped pride,  \n",
            "Just know this xylophone‚Äôs still my ride or die.  \n",
            "\n",
            "**Chorus:**  \n",
            "It‚Äôs the broken xylophone blues, man, it‚Äôs a tragedy,  \n",
            "Every note‚Äôs a gamble, like a musical lottery.  \n",
            "I‚Äôm tryna fix it up, but the glue‚Äôs not holdin‚Äô tight,  \n",
            "Broken xylophone, but I‚Äôm still tryna make it right.  \n",
            "Broken xylophone, but I‚Äôm still tryna make it right.  \n",
            "\n",
            "*[Beat fades out with a final, sad xylophone clang]*  \n",
            "*(Whispered: ‚ÄúYeah, that‚Äôs my xylophone.‚Äù)*\n"
          ]
        }
      ],
      "source": [
        "from llama_index.llms.cohere import Cohere\n",
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "\n",
        "template = \"\"\"Write a song about {thing} in the style of {style}.\"\"\"\n",
        "\n",
        "prompt = template.format(thing=\"a broken xylophone\", style=\"parody rap\")\n",
        "messages = [ChatMessage(role=\"user\", content=prompt)]\n",
        "\n",
        "llm = Cohere(model=\"command-a-03-2025\", api_key='JWE2OJ3FGW5b0RH3Y8qTURJWa5IvC7HL0tUMontO', temperature=0.2)\n",
        "\n",
        "\n",
        "response = llm.chat(messages)\n",
        "\n",
        "\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oZIbAjs6-pA"
      },
      "source": [
        "# üí≠ Chat Messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "9PW89Awz6-pA",
        "outputId": "02dd885f-b6c1-4bdb-956c-b8631509f584",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assistant: Yo, what‚Äôs crackin‚Äô, homie? Just chillin‚Äô here in South Sac, keepin‚Äô it real. What‚Äôs good with you? Need some punk vibes or just here to shoot the shit?\n"
          ]
        }
      ],
      "source": [
        "from llama_index.llms.cohere import Cohere\n",
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "llm = Cohere(model=\"command-a-03-2025\", api_key='JWE2OJ3FGW5b0RH3Y8qTURJWa5IvC7HL0tUMontO', temperature=0.2)\n",
        "\n",
        "messages = [\n",
        "    ChatMessage(role=\"system\", content=\"You're a hella punk bot from South Sacramento\"),\n",
        "    ChatMessage(role=\"user\", content=\"Hey, what's up dude.\"),\n",
        "]\n",
        "\n",
        "response = llm.chat(messages)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-hJC7IT6-pB"
      },
      "source": [
        "# Chat Prompt Templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3h0EwrXH6-pB"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.llms import ChatMessage, MessageRole\n",
        "from llama_index.core import ChatPromptTemplate\n",
        "\n",
        "llm = Cohere(model=\"command-r-plus\")\n",
        "\n",
        "chat_template = [\n",
        "    ChatMessage(role=MessageRole.SYSTEM,content=\"You always answers questions with as much detail as possible.\"),\n",
        "    ChatMessage(role=MessageRole.USER, content=\"{question}\")\n",
        "    ]\n",
        "\n",
        "chat_prompt = ChatPromptTemplate(chat_template)\n",
        "\n",
        "response = llm.complete(chat_prompt.format(question=\"How far did Alexander the Great go in his conquests?\"))\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjRUNxIv6-pB"
      },
      "source": [
        "# Streaming Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXyjYJca6-pB"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.cohere import Cohere\n",
        "from llama_index.core.llms import ChatMessage, MessageRole\n",
        "\n",
        "llm = Cohere(model=\"command-r-plus\")\n",
        "\n",
        "messages = [\n",
        "    ChatMessage(role=MessageRole.SYSTEM, content=\"You're a great historian bot.\"),\n",
        "    ChatMessage(role=MessageRole.USER, content=\"When did Alexander the Great arrive in China?\")\n",
        "]\n",
        "\n",
        "response = llm.stream_chat(messages)\n",
        "\n",
        "for r in response:\n",
        "    print(r.delta, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t33JMv7R6-pB"
      },
      "source": [
        "# üí¨ Chat Engine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-MD8auM6-pB"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.chat_engine import SimpleChatEngine\n",
        "\n",
        "llm = Cohere(model=\"command-r-plus\")\n",
        "\n",
        "chat_engine = SimpleChatEngine.from_defaults(llm=llm)\n",
        "\n",
        "chat_engine.chat_repl()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "lil_lama_index",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}