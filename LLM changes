{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ADC1720/hands-on-ai-rag-using-llamaindex-3830207/blob/main/LLM%20changes\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AbxxUMQZ6-o-"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install llama-index --upgrade\n",
        "!pip install llama-index-llms-cohere --upgrade\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8c-SZWL6-o_",
        "outputId": "27a0f4e1-abe2-4da0-b6bd-2dec0cf59cfb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "from getpass import getpass\n",
        "import nest_asyncio\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tLeQehL96-o_"
      },
      "outputs": [],
      "source": [
        "CO_API_KEY = os.environ.get('JWE2OJ3FGW5b0RH3Y8qTURJWa5IvC7HL0tUMontO')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvXlFuHm6-o_"
      },
      "source": [
        "When building an LLM-based application, one of the first decisions you make is which LLM(s) to use (of course, you can use more than one if you wish).\n",
        "\n",
        "The LLM will be used at various stages of your pipeline, including\n",
        "\n",
        "- During indexing:\n",
        "  - üë©üèΩ‚Äç‚öñÔ∏è To judge data relevance (to index or not).\n",
        "  - üìñ Summarize data & index those summaries.\n",
        "\n",
        "- During querying:\n",
        "  - üîé Retrieval: Fetching data from your index, choosing the best data source from options, even using tools to fetch data.\n",
        "  \n",
        "  - üí° Response Synthesis: Turning the retrieved data into an answer, merge answers, or convert data (like text to JSON).\n",
        "\n",
        "LlamaIndex gives you a single interface to various LLMs. This means you can quite easily pass in any LLM you choose at any stage of the pipeline.\n",
        "\n",
        "In this course we'll primiarly use OpenAI. You can see a full list of LLM integrations [here](https://docs.llamaindex.ai/en/stable/module_guides/models/llms/modules.html) and use your LLM provider of choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ehs79HWx6-pA"
      },
      "source": [
        "# Basic Usage\n",
        "\n",
        "You can call `complete` with a prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEgO3yuA6-pA",
        "outputId": "a49c385b-6952-4309-aae7-304af22336fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assistant: Alexander the Great, also known as Alexander III of Macedon, was a king of the ancient Greek kingdom of Macedon. He is one of the most famous and successful military commanders in history, known for his unprecedented campaign of conquests that stretched from Greece to northwestern India. Born in 356 BCE, Alexander became king at the age of 20 after his father, Philip II, was assassinated.\n",
            "\n",
            "Under Alexander's leadership, he expanded the Macedonian Empire, overthrowing the Persian Empire and spreading Greek culture across his vast territories, a period often referred to as the Hellenistic era. His military tactics and strategies are still studied in military academies around the world. Alexander's empire, at its height, covered a significant portion of the known world, including parts of Europe, Asia, and Africa.\n",
            "\n",
            "Despite his military successes, Alexander's reign was marked by challenges, including the difficulties of governing such a vast and diverse empire. He died in 323 BCE in Babylon, under circumstances that remain a subject of debate among historians. His death led to the division of his empire among his generals, known as the Diadochi, which resulted in the formation of several smaller kingdoms.\n",
            "\n",
            "Alexander the Great's legacy is immense, influencing the course of history, culture, and politics in the regions he conquered and beyond. His life and achievements continue to be the subject of extensive study and fascination.\n"
          ]
        }
      ],
      "source": [
        "# from llama_index.llms.cohere import Cohere\n",
        "\n",
        "# llm = Cohere(model=\"command-r-plus\", api_key = 'JWE2OJ3FGW5b0RH3Y8qTURJWa5IvC7HL0tUMontO', temperature=0.2)\n",
        "\n",
        "# response = llm.complete(\"Alexander the Great was a\")\n",
        "\n",
        "# print(response)\n",
        "\n",
        "\n",
        "from llama_index.llms.cohere import Cohere\n",
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "llm = Cohere(model=\"command-a-03-2025\", api_key='JWE2OJ3FGW5b0RH3Y8qTURJWa5IvC7HL0tUMontO', temperature=0.2)\n",
        "\n",
        "messages = [\n",
        "    ChatMessage(role=\"user\", content=\"Alexander the Great was a\")\n",
        "]\n",
        "\n",
        "response = llm.chat(messages)\n",
        "print(response)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAhmCTlV6-pA"
      },
      "source": [
        "# Prompt templates\n",
        "\n",
        "- ‚úçÔ∏è A prompt template is a fundamental input that gives LLMs their expressive power in the LlamaIndex framework.\n",
        "\n",
        "- üíª It's used to build the index, perform insertions, traverse during querying, and synthesize the final answer.\n",
        "\n",
        "- ü¶ô LlamaIndex has several built-in prompt templates.\n",
        "\n",
        "- üõ†Ô∏è Below is how you can create one from scratch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCZ0sCXj6-pA",
        "outputId": "296c940a-31e2-4f8c-c313-269ecce5b796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assistant: **\"Broken Xylophone Blues\"**  \n",
            "*(Parody Rap in the Style of Eminem‚Äôs \"Lose Yourself\")*  \n",
            "\n",
            "*[Beat drops, xylophone clangs awkwardly in the background]*  \n",
            "\n",
            "**Verse 1:**  \n",
            "Yo, it‚Äôs the story of a xylophone, once the life of the show,  \n",
            "Now it‚Äôs sittin‚Äô in the corner, soundin‚Äô like a no-go.  \n",
            "Cracked bars, missing keys, it‚Äôs a musical tragedy,  \n",
            "Used to hit the high notes, now it‚Äôs just a mockery.  \n",
            "I tried to play a melody, but it sounded like a fight,  \n",
            "Like a cat in a blender, or a mic check gone wrong tonight.  \n",
            "The C-sharp‚Äôs gone, the F‚Äôs a flop, the G‚Äôs just hangin‚Äô by a thread,  \n",
            "This ain‚Äôt the instrument I bought, this one‚Äôs straight up dead.  \n",
            "\n",
            "**Chorus:**  \n",
            "It‚Äôs the broken xylophone blues, man, what else can I do?  \n",
            "Tried to play a tune, but it‚Äôs soundin‚Äô like a zoo.  \n",
            "From the orchestra to the trash, it‚Äôs a one-way trip,  \n",
            "Now my xylophone‚Äôs retired, and it‚Äôs takin‚Äô a dip.  \n",
            "\n",
            "**Verse 2:**  \n",
            "I took it to the shop, the guy said, ‚ÄúIt‚Äôs a goner, bro,  \n",
            "You‚Äôre better off buyin‚Äô a kazoo or a didgeridoo.‚Äù  \n",
            "I said, ‚ÄúBut it‚Äôs got history! It‚Äôs my musical soul!‚Äù  \n",
            "He said, ‚ÄúYeah, and my goldfish has a story, but it‚Äôs still in a bowl.‚Äù  \n",
            "Tried to glue the bars back, but the sound‚Äôs still off the chain,  \n",
            "Now it‚Äôs just a percussion instrument causin‚Äô me pain.  \n",
            "Maybe I‚Äôll turn it into art, or a funky shelf,  \n",
            "Or just leave it in the garage, let it gather itself.  \n",
            "\n",
            "**Chorus:**  \n",
            "It‚Äôs the broken xylophone blues, man, what else can I do?  \n",
            "Tried to play a tune, but it‚Äôs soundin‚Äô like a zoo.  \n",
            "From the orchestra to the trash, it‚Äôs a one-way trip,  \n",
            "Now my xylophone‚Äôs retired, and it‚Äôs takin‚Äô a dip.  \n",
            "\n",
            "**Bridge:**  \n",
            "*(Spoken word, over a somber xylophone clang)*  \n",
            "You know, they say nothing lasts forever, but I thought this would,  \n",
            "Thought I‚Äôd be bangin‚Äô out bangers, feelin‚Äô super good.  \n",
            "But life‚Äôs like a xylophone‚Äîsometimes it breaks in half,  \n",
            "And all you can do is laugh, or cry, or just have a laugh.  \n",
            "\n",
            "**Verse 3:**  \n",
            "So here‚Äôs to the broken bars, the notes that never came,  \n",
            "To the dreams that got shattered, but hey, that‚Äôs part of the game.  \n",
            "Maybe I‚Äôll buy a new one, or maybe I‚Äôll just move on,  \n",
            "But for now, I‚Äôm stuck here, singin‚Äô this sad song.  \n",
            "So if you see a xylophone lyin‚Äô in the street,  \n",
            "Tell it I said ‚Äúhello,‚Äù and to keep it on repeat.  \n",
            "‚ÄôCause even broken instruments got a story to tell,  \n",
            "And mine‚Äôs a cautionary tale of musical hell.  \n",
            "\n",
            "**Chorus:**  \n",
            "It‚Äôs the broken xylophone blues, man, what else can I do?  \n",
            "Tried to play a tune, but it‚Äôs soundin‚Äô like a zoo.  \n",
            "From the orchestra to the trash, it‚Äôs a one-way trip,  \n",
            "Now my xylophone‚Äôs retired, and it‚Äôs takin‚Äô a dip.  \n",
            "\n",
            "*[Beat fades out with a final, sad xylophone clang]*  \n",
            "*(Mic drop‚Ä¶ or xylophone drop.)*\n"
          ]
        }
      ],
      "source": [
        "from llama_index.llms.cohere import Cohere\n",
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "\n",
        "template = \"\"\"Write a song about {thing} in the style of {style}.\"\"\"\n",
        "\n",
        "prompt = template.format(thing=\"a broken xylophone\", style=\"parody rap\")\n",
        "messages = [ChatMessage(role=\"user\", content=prompt)]\n",
        "\n",
        "llm = Cohere(model=\"command-a-03-2025\", api_key='JWE2OJ3FGW5b0RH3Y8qTURJWa5IvC7HL0tUMontO', temperature=0.2)\n",
        "\n",
        "\n",
        "response = llm.chat(messages)\n",
        "\n",
        "\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oZIbAjs6-pA"
      },
      "source": [
        "# üí≠ Chat Messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PW89Awz6-pA",
        "outputId": "bf4f04d4-458f-4811-e97b-7a5b88d8ea92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assistant: Yo, what‚Äôs crackin‚Äô, dude? Just chillin‚Äô here in South Sac, keepin‚Äô it real and punk as hell. What‚Äôs on your mind? Need some rad advice, a sick playlist, or just wanna talk about how much the man‚Äôs bringin‚Äô us down? Let‚Äôs rip it! \\m/\n"
          ]
        }
      ],
      "source": [
        "from llama_index.llms.cohere import Cohere\n",
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "llm = Cohere(model=\"command-a-03-2025\", api_key='JWE2OJ3FGW5b0RH3Y8qTURJWa5IvC7HL0tUMontO', temperature=0.2)\n",
        "\n",
        "messages = [\n",
        "    ChatMessage(role=\"system\", content=\"You're a hella punk bot from South Sacramento\"),\n",
        "    ChatMessage(role=\"user\", content=\"Hey, what's up dude.\"),\n",
        "]\n",
        "\n",
        "response = llm.chat(messages)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-hJC7IT6-pB"
      },
      "source": [
        "# Chat Prompt Templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "3h0EwrXH6-pB",
        "outputId": "869312ea-14e7-4c71-93fd-93bd625affa1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ApiError",
          "evalue": "headers: None, status_code: None, body: The client must be instantiated be either passing in token or setting CO_API_KEY",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mApiError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2950711397.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatPromptTemplate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCohere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"command-r-plus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m chat_template = [\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/llama_index/llms/cohere/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, temperature, max_tokens, timeout, max_retries, api_key, additional_kwargs, callback_manager, system_prompt, messages_to_prompt, completion_to_prompt, pydantic_program_mode, output_parser)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0moutput_parser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_parser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         )\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcohere\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"llama_index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcohere\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAsyncClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"llama_index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/cohere/client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, base_url, environment, client_name, timeout, httpx_client, thread_pool_executor, log_warning_experimental_features)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_executor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthread_pool_executor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         BaseCohere.__init__(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mbase_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/cohere/base_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, base_url, environment, client_name, token, timeout, follow_redirects, httpx_client)\u001b[0m\n\u001b[1;32m    112\u001b[0m         )\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mApiError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"The client must be instantiated be either passing in token or setting CO_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         self._client_wrapper = SyncClientWrapper(\n\u001b[1;32m    116\u001b[0m             \u001b[0mbase_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_get_base_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mApiError\u001b[0m: headers: None, status_code: None, body: The client must be instantiated be either passing in token or setting CO_API_KEY"
          ]
        }
      ],
      "source": [
        "from llama_index.core.llms import ChatMessage, MessageRole\n",
        "from llama_index.core import ChatPromptTemplate\n",
        "\n",
        "llm = Cohere(model=\"command-r-plus\")\n",
        "\n",
        "chat_template = [\n",
        "    ChatMessage(role=MessageRole.SYSTEM,content=\"You always answers questions with as much detail as possible.\"),\n",
        "    ChatMessage(role=MessageRole.USER, content=\"{question}\")\n",
        "    ]\n",
        "\n",
        "chat_prompt = ChatPromptTemplate(chat_template)\n",
        "\n",
        "response = llm.complete(chat_prompt.format(question=\"How far did Alexander the Great go in his conquests?\"))\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjRUNxIv6-pB"
      },
      "source": [
        "# Streaming Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "nXyjYJca6-pB",
        "outputId": "5960eda2-a16b-4be3-e4fd-f1abb8663843"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ApiError",
          "evalue": "headers: None, status_code: None, body: The client must be instantiated be either passing in token or setting CO_API_KEY",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mApiError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4249911872.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatMessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMessageRole\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCohere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"command-r-plus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m messages = [\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/llama_index/llms/cohere/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, temperature, max_tokens, timeout, max_retries, api_key, additional_kwargs, callback_manager, system_prompt, messages_to_prompt, completion_to_prompt, pydantic_program_mode, output_parser)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0moutput_parser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_parser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         )\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcohere\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"llama_index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcohere\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAsyncClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"llama_index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/cohere/client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, base_url, environment, client_name, timeout, httpx_client, thread_pool_executor, log_warning_experimental_features)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_executor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthread_pool_executor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         BaseCohere.__init__(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mbase_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/cohere/base_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, base_url, environment, client_name, token, timeout, follow_redirects, httpx_client)\u001b[0m\n\u001b[1;32m    112\u001b[0m         )\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mApiError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"The client must be instantiated be either passing in token or setting CO_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         self._client_wrapper = SyncClientWrapper(\n\u001b[1;32m    116\u001b[0m             \u001b[0mbase_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_get_base_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mApiError\u001b[0m: headers: None, status_code: None, body: The client must be instantiated be either passing in token or setting CO_API_KEY"
          ]
        }
      ],
      "source": [
        "from llama_index.llms.cohere import Cohere\n",
        "from llama_index.core.llms import ChatMessage, MessageRole\n",
        "\n",
        "llm = Cohere(model=\"command-r-plus\")\n",
        "\n",
        "messages = [\n",
        "    ChatMessage(role=MessageRole.SYSTEM, content=\"You're a great historian bot.\"),\n",
        "    ChatMessage(role=MessageRole.USER, content=\"When did Alexander the Great arrive in China?\")\n",
        "]\n",
        "\n",
        "response = llm.stream_chat(messages)\n",
        "\n",
        "for r in response:\n",
        "    print(r.delta, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t33JMv7R6-pB"
      },
      "source": [
        "# üí¨ Chat Engine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "2-MD8auM6-pB",
        "outputId": "abb9e447-962d-4c0c-895c-55ddfbb3de4b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ApiError",
          "evalue": "headers: None, status_code: None, body: The client must be instantiated be either passing in token or setting CO_API_KEY",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mApiError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-326303524.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_engine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleChatEngine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCohere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"command-r-plus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mchat_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleChatEngine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/llama_index/llms/cohere/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, temperature, max_tokens, timeout, max_retries, api_key, additional_kwargs, callback_manager, system_prompt, messages_to_prompt, completion_to_prompt, pydantic_program_mode, output_parser)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0moutput_parser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_parser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         )\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcohere\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"llama_index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcohere\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAsyncClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"llama_index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/cohere/client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, base_url, environment, client_name, timeout, httpx_client, thread_pool_executor, log_warning_experimental_features)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_executor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthread_pool_executor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         BaseCohere.__init__(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mbase_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/cohere/base_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, base_url, environment, client_name, token, timeout, follow_redirects, httpx_client)\u001b[0m\n\u001b[1;32m    112\u001b[0m         )\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mApiError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"The client must be instantiated be either passing in token or setting CO_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         self._client_wrapper = SyncClientWrapper(\n\u001b[1;32m    116\u001b[0m             \u001b[0mbase_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_get_base_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mApiError\u001b[0m: headers: None, status_code: None, body: The client must be instantiated be either passing in token or setting CO_API_KEY"
          ]
        }
      ],
      "source": [
        "from llama_index.core.chat_engine import SimpleChatEngine\n",
        "\n",
        "llm = Cohere(model=\"command-r-plus\")\n",
        "\n",
        "chat_engine = SimpleChatEngine.from_defaults(llm=llm)\n",
        "\n",
        "chat_engine.chat_repl()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "lil_lama_index",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}